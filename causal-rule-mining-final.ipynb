{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "file = 'processed_data.csv'\n",
    "df =pd.read_csv(r\"./data/%s\" % file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables = ['ApplicationType', 'binned_CreditScore', 'LoanGoal', 'binned_RequestedAmount', \n",
    "                      'binned_FirstWithdrawalAmount', 'binned_MonthlyCost', 'binned_NoOfTerms', 'binned_NumberOfOffers']\n",
    "controllable_variables =  ['binned_FirstWithdrawalAmount', 'binned_MonthlyCost', 'binned_NoOfTerms', 'binned_NumberOfOffers']\n",
    "target = 'Selected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #for sorting\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['Z'] = df[target]\n",
    "data['notZ'] = 1 - data['Z']\n",
    "new_controllable_variables=[]\n",
    "\n",
    "# from https://stackoverflow.com/questions/5967500/how-to-correctly-sort-a-string-with-a-number-inside\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
    "\n",
    "\n",
    "controllable_feature_list =[] # double array, consisting of lists of related (exlusive) features\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_encoding(column_name: str):\n",
    "    \"\"\" For every unique value in the original column a seperate column will be added in the binary dataframe called 'data',\n",
    "    and when the column only has 2 values, only 1 column will be created,\n",
    "    where for both the name of the column is the name of the column of origin and the selected value combined,\n",
    "    where for both 1 represents the value in the original column was equal to the selected value and 0 otherwise,\n",
    "    the function returns for binary columns the encoder and for the other columns, the names of the original values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # There is only 1 column needed to decifer the original value if the original column had only 2 values\n",
    "    if len(df[column_name].unique()) == 2:\n",
    "        binary_list = []\n",
    "        value = df[column_name].unique()[0]\n",
    "        encoding = [[df[column_name].unique()[0],1],[df[column_name].unique()[1],0]]\n",
    "        for i in df[column_name]:\n",
    "            if i == value:\n",
    "                binary_list.append(1)\n",
    "            else:\n",
    "                binary_list.append(0)\n",
    "        data[column_name +\"_\"+ str(value)] = binary_list\n",
    "        if column_name in controllable_variables:\n",
    "            new_controllable_variables.append(column_name +\"_\"+ str(value))\n",
    "            cfl=[column_name +\"_\"+ value]\n",
    "            controllable_feature_list.append(cfl)\n",
    "        return encoding\n",
    "        \n",
    "    if len(df[column_name].unique()) > 2:\n",
    "        cfl=[]\n",
    "        values=df[column_name].unique()\n",
    "        sorted_values=sorted(values,key=natural_keys)\n",
    "        for value in sorted_values:\n",
    "            binary_list = []\n",
    "            for i in df[column_name]:\n",
    "                if i == value:\n",
    "                    binary_list.append(1)\n",
    "                else:\n",
    "                    binary_list.append(0)\n",
    "            data[column_name +\"_\"+ value] = binary_list\n",
    "            if column_name in controllable_variables:\n",
    "                new_controllable_variables.append(column_name +\"_\"+ value)\n",
    "                cfl.append(column_name +\"_\"+ value)\n",
    "        if cfl!=[]:\n",
    "            controllable_feature_list.append(cfl)\n",
    "        return list(df[column_name].unique())\n",
    "    \n",
    "# Execute the one-hot encoding and retrieve the encoding scheme\n",
    "encoding_scheme = {}\n",
    "\n",
    "for i in selected_variables:\n",
    "    encoding_scheme[i] = one_hot_encoding(i)\n",
    "    \n",
    "#Set the variables to one-hot encoded column_names and pop the target variable\n",
    "new_selected_variables = list(data.columns)\n",
    "new_selected_variables.pop(0)\n",
    "new_selected_variables.pop(0)\n",
    "print(new_controllable_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions and Classes for the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "from re import X\n",
    "\n",
    "\n",
    "rank=defaultdict(int)\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, itemName, itemSet, parentNodeList,d):\n",
    "        self.itemName = itemName\n",
    "        self.itemSet = itemSet\n",
    "        self.lsupp = 0\n",
    "        self.parentlist = parentNodeList\n",
    "        self.children = set ()\n",
    "        self.depth=d;\n",
    "\n",
    "\n",
    "    def display(self, ind=1):\n",
    "        print('  ' * ind, self.itemSet, ' ', self.lsupp)\n",
    "        for child in list(self.children):\n",
    "            child.display(ind+1)\n",
    "\n",
    "def generateNextLevelTree(treeNode, root, level, RHS, dataset):\n",
    "    if treeNode.depth <level-1: \n",
    "        for c in treeNode.children:\n",
    "            generateNextLevelTree(c, root, level, RHS, dataset)\n",
    "    else: \n",
    "        if treeNode.depth==level-1:\n",
    "            for c1 in treeNode.children:\n",
    "                for c2 in treeNode.children:\n",
    "                    if c1==c2: continue\n",
    "                    itemSet=c1.itemSet.union(c2.itemSet)\n",
    "                    if (len(itemSet)==0):\n",
    "                        print(\"non-empty itemset expected\")\n",
    "                        sys.exit()\n",
    "                    if contain(root,root,itemSet)!=None:\n",
    "                        continue # similar node has been created already\n",
    "                    \n",
    "                    stop = False\n",
    "                \n",
    "                    parentNodeList = list()\n",
    "                    for v in itemSet:\n",
    "                        itemSetminV =itemSet.copy()\n",
    "                        itemSetminV.remove(v)\n",
    "                        if len(itemSetminV)==0:\n",
    "                             continue;\n",
    "                        n = contain(root,root,itemSetminV)\n",
    "                        if n== None:\n",
    "                            stop = True\n",
    "                            break\n",
    "                        else:\n",
    "                            parentNodeList.append(n)\n",
    "                    if stop: continue\n",
    "                    \n",
    "                    itemName= itemSet - c1.itemSet\n",
    "                    itemName=itemName.pop()\n",
    "                    if not c1 in parentNodeList:\n",
    "                        parentNodeList.append(c1)\n",
    "                    newNode = Node(itemName,itemSet,parentNodeList,c1.depth+1)\n",
    "                    newNode.lsupp =local_support(itemSet,RHS,dataset)\n",
    "                    c1.children.add(newNode)\n",
    "\n",
    "\n",
    "def contain(treeNode, root, itemSet: set):\n",
    "    if itemSet == set():\n",
    "        # should be impossible\n",
    "        treeNode.display(10)\n",
    "        sys.exit()\n",
    "    if treeNode == root:\n",
    "        for c in treeNode.children:\n",
    "            n=contain(c,root,itemSet)\n",
    "            if n!= None:\n",
    "                return n\n",
    "    else:\n",
    "        if treeNode.itemSet == itemSet:\n",
    "            return treeNode\n",
    "        else: \n",
    "            if treeNode.itemName not in itemSet:\n",
    "                return None\n",
    "            for c in treeNode.children:\n",
    "               n=contain(c,root,itemSet)\n",
    "               if n!= None :\n",
    "                  return n\n",
    "    return None\n",
    "\n",
    "\n",
    "def prune(T,minsup):\n",
    "    if T.depth>0 and T.lsupp < minsup:\n",
    "        for parent in T.parentlist:\n",
    "            parentchildren=parent.children\n",
    "            parentchildren.discard(T)\n",
    "    else:\n",
    "        for c in T.children.copy():\n",
    "            prune(c,minsup)\n",
    "\n",
    "#observation 2 from paper by Li et al.\n",
    "def pruneSameSupport(T):\n",
    "    if T.depth>0 :\n",
    "        same=True\n",
    "        for parent in T.parentlist:\n",
    "            if T.lsupp!=parent.lsupp:\n",
    "                same=False\n",
    "                break\n",
    "        if same:\n",
    "            parentchildren=parent.children\n",
    "            parentchildren.discard(T)\n",
    "        else:\n",
    "           for c in T.children.copy():\n",
    "            pruneSameSupport(c) \n",
    "    else:\n",
    "        for c in T.children.copy():\n",
    "            pruneSameSupport(c)\n",
    "\n",
    "def removeNode(T, itemSet):\n",
    "    if T.itemSet==itemSet:\n",
    "        for parent in T.parentlist:\n",
    "            parentchildren=parent.children\n",
    "            parentchildren.discard(T)\n",
    "        exit\n",
    "    else:\n",
    "        for c in T.children.copy():\n",
    "            removeNode(c,itemSet)\n",
    "\n",
    "def generateAssocationRules(T,k,RHS,dataset):\n",
    "    LHS_AR=[]\n",
    "    if T.depth==k:\n",
    "        if association_test(list(T.itemSet),RHS,dataset):\n",
    "            LHS_AR.append(list(T.itemSet))\n",
    "    else:\n",
    "        i=0\n",
    "        for c in T.children:\n",
    "            LHS_AR=LHS_AR+generateAssocationRules(c,k,RHS,dataset)\n",
    "    return LHS_AR\n",
    "  \n",
    "#RHS plays the role of z\n",
    "def support_counter(var_list,RHS, dataset):\n",
    "    #Create for the variables in the list a mask where the values all should be equal to 1\n",
    "    p = 1\n",
    "    for v in var_list:\n",
    "        p = p & (dataset[v] ==1)\n",
    "    \n",
    "    supp_p_z = dataset[p][RHS].sum()\n",
    "    supp_p_not_z = dataset[p][RHS].count() - supp_p_z\n",
    "    \n",
    "    not_p = ~p\n",
    "    supp_not_p_z = dataset[not_p][RHS].sum()\n",
    "    supp_not_p_not_z = dataset[not_p][RHS].count() - dataset[not_p][RHS].sum()\n",
    "    return supp_p_z, supp_p_not_z, supp_not_p_not_z, supp_not_p_z\n",
    "\n",
    "# RHS plays the role of z\n",
    "def local_support(var_list, RHS, dataset) -> float:\n",
    "    \"\"\" Uses the support_counter() function to calculate the local support\"\"\"\n",
    "    \n",
    "    supp_p_z, supp_p_not_z, supp_not_p_not_z, supp_not_p_z = support_counter(var_list, RHS, dataset)\n",
    "    supp_z = supp_p_z + supp_not_p_z\n",
    "    l_supp = supp_p_z/supp_z \n",
    "    \n",
    "    return l_supp\n",
    "\n",
    "# check support of rule var_list -> RHS, where RHS plays the role of z below\n",
    "def computeMetrics (var_list, RHS, dataset):\n",
    "    supp_p_z, supp_p_not_z, supp_not_p_not_z, supp_not_p_z = support_counter(var_list, RHS, dataset)\n",
    "    supp = supp_p_z / dataset.shape[0]\n",
    "    supp_p = supp_p_z + supp_p_not_z\n",
    "    supp_z = supp_p_z + supp_not_p_z \n",
    "    conf = supp / supp_p\n",
    "    lift = supp_p_z / (supp_p*supp_z)\n",
    "    return supp, conf, lift\n",
    "\n",
    "def association_test(var_list: list, RHS, dataset) -> bool:\n",
    "    \"\"\" An association is significant if the oddsratio is with 95% confidence higher than 1. In this function the association \n",
    "    between the set of variables in var_list and the target variable RHS is tested. Returns True when there is a significant\n",
    "    association and False otherwise\n",
    "    \"\"\"\n",
    "    supp_p_z, supp_p_not_z, supp_not_p_not_z, supp_not_p_z = support_counter(var_list, RHS, dataset)\n",
    "    #Haldane-Anscombe correction for when one of the support is 0 (the correction is just adding 0.5 to all)\n",
    "    supp_p_z, supp_p_not_z= supp_p_z + 0.5, supp_p_not_z + 0.5\n",
    "    supp_not_p_not_z, supp_not_p_z = supp_not_p_not_z + 0.5 , supp_not_p_z + 0.5\n",
    "    \n",
    "    #95% confidence -> z' = 1.96, odr = oddsratio(p -> z) and lb = lower bound\n",
    "    odr =(supp_p_z * supp_not_p_not_z)/(supp_p_not_z * supp_not_p_z)\n",
    "    \n",
    "    lb_or= math.exp(np.log(odr)- (1.96*(math.sqrt((1/supp_p_z) + (1/supp_p_not_z) + (1/supp_not_p_z) + (1/supp_not_p_not_z)))))\n",
    "    if lb_or > 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def find_exclusive_variables(LHS:list, X:list, dataset, min_l_supp = 0) -> list:\n",
    "    \"\"\"\n",
    "    Requires input of the lefthand side of the association rule and the list of variables which can be exclusive from the \n",
    "    lefthand side (X). For each element in X it is tested whether the element is exclusive from the LHS list. The function \n",
    "    returns the set of variables that are exclusive in list E. \n",
    "    \"\"\"\n",
    "    \n",
    "    E = []\n",
    "    P = LHS\n",
    "    if P==[]:\n",
    "        return []\n",
    "\n",
    "    p = 1\n",
    "    for v in LHS:\n",
    "        p = p & (dataset[v] ==1)        \n",
    "    \n",
    "\n",
    "    for variable in X:\n",
    "        Q = variable\n",
    "        if Q not in P:\n",
    "            q = (dataset[Q] == 1)\n",
    "            mask_p_q = p & q\n",
    "            mask_not_p_q = (~p) & q\n",
    "            supp_p_q = dataset[mask_p_q][Q].count()\n",
    "            supp_not_p_q = dataset[mask_not_p_q][Q].count()\n",
    "            if supp_p_q <= min_l_supp:\n",
    "                E.append(Q)\n",
    "            elif supp_p_q <= min_l_supp:\n",
    "                E.append(Q)  \n",
    "    return E\n",
    "\n",
    "def create_fair_dataset(dataset, LHS_set:list, RHS, C:list):\n",
    "    \"\"\"\n",
    "    Matches tuples and returns n11, n12, n21, n22\n",
    "    \"\"\"\n",
    "\n",
    "    all_relevant_vars = C + [RHS] + LHS_set\n",
    "    \n",
    "    n12, n21 = 0, 0\n",
    "    \n",
    "    P = LHS_set\n",
    "\n",
    "    df_C = dataset[all_relevant_vars].groupby(C)\n",
    "    for name, df_c in df_C:\n",
    "        p =1\n",
    "        for i in P:\n",
    "            p = p & (df_c[i] == 1)\n",
    "        df_p = df_c[all_relevant_vars][p]\n",
    "        df_not_p = df_c[all_relevant_vars][~p]\n",
    "        if df_p.count()[0]+df_not_p.count()[0]!=df_c.count()[0]:\n",
    "            print(\"missing rows counted\")\n",
    "            sys.exit()\n",
    "        count = min (df_p.count()[0],df_not_p.count()[0])\n",
    "        if count==0:\n",
    "            continue\n",
    "        df_p_sample = df_p.sample(count)\n",
    "        df_not_p_sample = df_not_p.sample(count)\n",
    "        pz_count= df_p_sample[RHS].sum()\n",
    "        pnotz_count = count - df_p_sample[RHS].sum()\n",
    "        notpz_count= df_not_p_sample[RHS].sum()\n",
    "        notpnotz_count= count - df_not_p_sample[RHS].sum()\n",
    "        if (pz_count <= notpnotz_count):\n",
    "            n12+=pz_count\n",
    "            n21+=notpz_count\n",
    "        else:\n",
    "            n12+=notpnotz_count\n",
    "            n21+=pnotz_count\n",
    "\n",
    "\n",
    "    return n12, n21\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_l_supp = 0.01 #Based of page 18\n",
    "RHS='Z'\n",
    "RHS_set = [RHS]\n",
    "max_k = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "pRc = []   # stores the positive causal association rules found with fair data sets\n",
    "\n",
    "Rc_or = dict() # odds ratio\n",
    "\n",
    "Rc_supp = dict() # support\n",
    "\n",
    "Rc_conf = dict() # confidence\n",
    "\n",
    "Rc_lift = dict() # lift\n",
    "\n",
    "RcCHM = [] # stores the causal association rules found with orginal data sets and odds ratios/ Cochlan-Haenszel-Mantel metric\n",
    "\n",
    "#2\n",
    "main_root = Node('Null',{},[],0)\n",
    "\n",
    "for v in new_selected_variables:\n",
    "    n=Node(v,{v},[main_root],1)\n",
    "    main_root.children.add(n)\n",
    "    rank[v]=data[v].sum()\n",
    "    n.lsupp =local_support({v},RHS,data)\n",
    "\n",
    "\n",
    "#4\n",
    "prune(main_root,min_l_supp)\n",
    "\n",
    "X = [ c.itemName for c in main_root.children]\n",
    "\n",
    "I = set()\n",
    "for variable in X:\n",
    "    #If the function does not return True:\n",
    "    if not association_test([variable],RHS, data):\n",
    "        I.add(variable)\n",
    "        \n",
    "\n",
    "#7 \n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#8\n",
    "while k <= max_k:\n",
    "    found= False\n",
    "#9 LHS_AR stores the sets of variables of the LHS of the k-th level\n",
    "    LHS_AR= generateAssocationRules(main_root,k,RHS,data)\n",
    "\n",
    "#11\n",
    "    for LHS in LHS_AR:\n",
    "        E = find_exclusive_variables(LHS, X, data)\n",
    "#12     \n",
    "        C = [variable for variable in X if variable not in I \n",
    "                                         if variable not in E\n",
    "                                         if variable not in LHS]\n",
    "\n",
    "#13\n",
    "        n12, n21 = create_fair_dataset(data, LHS, RHS, C)\n",
    "#14 calculate Odds Ratio, page 10 if zero > count = 1 to evade infinite odds ratios\n",
    "        if n21 == 0:\n",
    "            n21 = 1\n",
    "        if n12 == 0:\n",
    "            n12 = 1\n",
    "            \n",
    "        O_ratio = n12/n21\n",
    "        # if lowerbound odds ratio (lb_or) > 1 with confidence 955 (z = 1.96)\n",
    "        lb_or = math.exp(np.log(O_ratio)- (1.96*(math.sqrt((1/(n12)) + (1/(n21))))))\n",
    "        if lb_or > 1:\n",
    "#15         \n",
    "            found= True\n",
    "            pRc.append(LHS)\n",
    "#16\n",
    "            supp, conf, lift= computeMetrics(LHS,RHS,data)\n",
    "            print ('------------------')\n",
    "            print(LHS, '-> Z is a positive CR')\n",
    "            print ('odds ratio', O_ratio)\n",
    "            print ('support: ', supp)\n",
    "            print ('confidence: ', conf)\n",
    "            print ('lift: ', lift)\n",
    "            print ('------------------')\n",
    "            Rc_or[tuple(LHS)]=O_ratio\n",
    "            Rc_supp[tuple(LHS)]=supp\n",
    "            Rc_conf[tuple(LHS)]=conf\n",
    "            Rc_lift[tuple(LHS)]=lift\n",
    " \n",
    "            \n",
    "        if found: # prune LHS from prefix tree\n",
    "            removeNode(main_root,set(LHS))\n",
    "            n=contain(main_root,main_root,set(LHS))\n",
    "            if n!=None:\n",
    "                print(\"Error: tree contains removed node!\")\n",
    "                n.display(1)\n",
    "                sys.exit()\n",
    "       \n",
    "            \n",
    "    if not found:\n",
    "        print (\"No CR rules found at level \",k)\n",
    "\n",
    "    # No nodes should be added after the max tree depth has been reached\n",
    "    if k == max_k:\n",
    "        break\n",
    "    \n",
    "    print('\\n'+'adding level ', k, 'nodes to the tree') \n",
    "\n",
    "#19\n",
    "    generateNextLevelTree(main_root,main_root,k,RHS,data)  \n",
    "    prune(main_root,min_l_supp)\n",
    "    pruneSameSupport(main_root)\n",
    "    k += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('postive causal rule ; odds ratio ; confidence ; support ; lift ' )\n",
    "for LHS in pRc:\n",
    "    print(LHS, '-> Z is a CR ; ', Rc_or[tuple(LHS)], ';', Rc_conf[tuple(LHS)], ';', Rc_supp[tuple(LHS)], ';', Rc_lift[tuple(LHS)] ,'\\n' )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_l_supp = 0.01 #Based of page 18\n",
    "RHS='notZ'\n",
    "RHS_set = [RHS]\n",
    "max_k = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "nRc = []   # stores the negataive causal association rules found with fair data sets\n",
    "\n",
    "nRc_or = dict() # odds ratio\n",
    "\n",
    "nRc_supp = dict() # support\n",
    "\n",
    "nRc_conf = dict() # confidence\n",
    "\n",
    "nRc_lift = dict() # lift\n",
    "\n",
    "\n",
    "#2\n",
    "nmain_root = Node('Null',{},[],0)\n",
    "\n",
    "for v in new_selected_variables:\n",
    "    n=Node(v,{v},[nmain_root],1)\n",
    "    nmain_root.children.add(n)\n",
    "    rank[v]=data[v].sum()\n",
    "    n.lsupp =local_support({v},RHS,data)\n",
    "\n",
    "\n",
    "#4\n",
    "prune(nmain_root,min_l_supp)\n",
    "\n",
    "X = [ c.itemName for c in nmain_root.children]\n",
    "\n",
    "I = set()\n",
    "for variable in X:\n",
    "    #If the function does not return True:\n",
    "    if not association_test([variable],RHS, data):\n",
    "        I.add(variable)\n",
    "        \n",
    "\n",
    "#7 \n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#8\n",
    "while k <= max_k:\n",
    "    found= False\n",
    "#9 LHS_AR stores the sets of variables of the LHS of the k-th level\n",
    "    LHS_AR= generateAssocationRules(nmain_root,k,RHS,data)\n",
    "\n",
    "#11\n",
    "    for LHS in LHS_AR:\n",
    "        E = find_exclusive_variables(LHS, X, data)\n",
    "#12     \n",
    "        C = [variable for variable in X if variable not in I \n",
    "                                         if variable not in E\n",
    "                                         if variable not in LHS]\n",
    "\n",
    "#13\n",
    "        n12, n21 = create_fair_dataset(data, LHS, RHS, C)\n",
    "#14 calculate Odds Ratio, page 10 if zero > count = 1 to evade infinite odds ratios\n",
    "        if n21 == 0:\n",
    "            n21 = 1\n",
    "        if n12 == 0:\n",
    "            n12 = 1\n",
    "            \n",
    "        O_ratio = n12/n21\n",
    "        # if lowerbound odds ratio (lb_or) > 1 with confidence 955 (z = 1.96)\n",
    "        lb_or = math.exp(np.log(O_ratio)- (1.96*(math.sqrt((1/(n12)) + (1/(n21))))))\n",
    "        if lb_or > 1:\n",
    "#15         \n",
    "            found= True\n",
    "            nRc.append(LHS)\n",
    "#16\n",
    "            supp, conf, lift= computeMetrics(LHS,RHS,data)\n",
    "            print ('------------------')\n",
    "            print(LHS, '-> !Z is a negative CR')\n",
    "            print ('odds ratio', O_ratio)\n",
    "            print ('support: ', supp)\n",
    "            print ('confidence: ', conf)\n",
    "            print ('lift: ', lift)\n",
    "            print ('------------------')\n",
    "            nRc_or[tuple(LHS)]=O_ratio\n",
    "            nRc_supp[tuple(LHS)]=supp\n",
    "            nRc_conf[tuple(LHS)]=conf\n",
    "            nRc_lift[tuple(LHS)]=lift\n",
    "\n",
    "            \n",
    "        if found:\n",
    "            removeNode(main_root,set(LHS))\n",
    "            n=contain(main_root,main_root,set(LHS))\n",
    "            if n!=None:\n",
    "                print(\"Error: tree contains removed node!\")\n",
    "                n.display(1)\n",
    "                sys.exit()\n",
    "       \n",
    "            \n",
    "    if not found:\n",
    "        print (\"No CR rules found at level \",k)\n",
    "\n",
    "    # No nodes should be added after the max tree depth has been reached\n",
    "    if k == max_k:\n",
    "        break\n",
    "   \n",
    "    print('\\n'+'adding level ', k, 'nodes to the tree') \n",
    "\n",
    "#19\n",
    "    generateNextLevelTree(main_root,main_root,k,RHS,data)  \n",
    "    prune(main_root,min_l_supp)\n",
    "    pruneSameSupport(main_root)\n",
    "    k += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('negative causal rule ; odds ratio ; confidence ; support ; lift ' )\n",
    "for LHS in nRc:\n",
    "    print(LHS, '-> !Z is a CR ; ', nRc_or[tuple(LHS)], ';', nRc_conf[tuple(LHS)], ';', nRc_supp[tuple(LHS)], ';', nRc_lift[tuple(LHS)] , '\\n' )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return true if a positive causal rule can be applied to the case, so the case does not (fully) satisfy the LHS and LHS can be completely changed (is fully controllable)\n",
    "def evaluatepositivecausalrule(LHS, case)-> bool:\n",
    "    for I in LHS:\n",
    "        if case[I]!=1 and I in new_controllable_variables:\n",
    "            return True\n",
    "    return False    \n",
    "\n",
    "# return true if a negative causal rule applies to the case with the positive features in pimpact, so the case + pimpact satisfies the LHS and the LHS can be completely changed (is fully controllable)\n",
    "def evaluatenegativecausalrule(LHS, pimpact, case)-> bool:\n",
    "    for I in LHS:\n",
    "        if case[I]!=1 and I not in pimpact:\n",
    "            return False\n",
    "        if  not (I in new_controllable_variables):\n",
    "            return False\n",
    "    return True    \n",
    "\n",
    "def positiveimpact(LHS,case)->list:\n",
    "    plist=[]\n",
    "    for I in LHS:\n",
    "        if case[I]!=1 and I in new_controllable_variables:\n",
    "            plist.append(I)\n",
    "    return plist   \n",
    "\n",
    "def negativeimpact(LHS,pimpact,case)->list:\n",
    "    nlist=[]\n",
    "    for I in LHS:\n",
    "        if (case[I]==1 or I in pimpact) and I in new_controllable_variables:\n",
    "            nlist.append(I)\n",
    "    return nlist   \n",
    "\n",
    "def exclusive(f):\n",
    "    l=[]\n",
    "    for cfl in controllable_feature_list:\n",
    "        if f in cfl:\n",
    "            l = cfl[:]\n",
    "            l.remove(f)\n",
    "            return l\n",
    "    return l\n",
    "\n",
    "def areexclusive(f1,f2):\n",
    "    if f1==f2: \n",
    "        return False\n",
    "    for cfl in controllable_feature_list:\n",
    "        if f1 in cfl and f2 in cfl:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def distance(f1,f2):\n",
    "    for cfl in controllable_feature_list:\n",
    "        if f1 in cfl and f2 in cfl:\n",
    "            return abs(cfl.index(f1)-cfl.index(f2))\n",
    "    return -1\n",
    "\n",
    "def alternative(f,case,Fpos,Fneg):\n",
    "    for fp in Fpos:\n",
    "        if areexclusive(f,fp):\n",
    "            return fp\n",
    "    E=exclusive(f)\n",
    "    lowestdist=len(E) # distance\n",
    "    closestf=None\n",
    "    l=[ f for f in E if (f not in Fneg)]\n",
    "\n",
    "    for fn in l:\n",
    "        d=distance(f,fn) \n",
    "        if d<lowestdist:\n",
    "            lowestdist=d\n",
    "            closestf=fn\n",
    "    return closestf\n",
    "\n",
    "def prevalternative(f,case):\n",
    "    E=exclusive(f)\n",
    "    for e in E:\n",
    "        if case[e]==1:\n",
    "            return e\n",
    "    return None\n",
    "\n",
    "def resolveconflicts(Fpos,Fprev):\n",
    "    toremove=[]\n",
    "    for f1 in Fpos:\n",
    "        for f2 in Fpos:\n",
    "            if areexclusive(f1,f2):\n",
    "                for f in Fprev:\n",
    "                    if areexclusive(f,f1):\n",
    "                        if distance(f,f1)<distance(f,f2):\n",
    "                            toremove.append(f2)\n",
    "                        else:\n",
    "                            toremove.append(f1)\n",
    "    Fposnew= [ f for f in Fpos if (f not in toremove) ]\n",
    "    return Fposnew\n",
    "\n",
    "def findoutcomealternativecase(dataset,case,Fneg,Falt,Fprev,Fpos):\n",
    "    mask=1\n",
    "    for v in new_selected_variables:\n",
    "        if v in Fneg or v in Falt or v in Fprev or v in Fpos:\n",
    "            continue\n",
    "        if case[v]==1:\n",
    "            mask=mask & (dataset[v]==1)\n",
    "        else:\n",
    "            mask=mask & (data[v]==0)\n",
    "    for f in Fneg:\n",
    "        mask=mask & (dataset[f]==0)\n",
    "    for f in Falt:\n",
    "        mask=mask & (dataset[f]==1)\n",
    "    for f in Fprev:\n",
    "        mask=mask & (dataset[f]==0)\n",
    "    for f in Fpos:\n",
    "        mask=mask & (dataset[f]==1)\n",
    "    alternativeoutcomes=dataset[mask]\n",
    "    print ('number of masked data rows ', dataset[mask].count()[0])\n",
    "    print ('number of times alternative case is already present', dataset[mask]['Z'].count())\n",
    "    print ('number of times alternative case has positive outcome', dataset[mask]['Z'].sum())\n",
    "    print ('number of times alternative case has negative outcome', (dataset[mask]['Z']==0).sum()) \n",
    "    return dataset[mask]['Z'].count(), dataset[mask]['Z'].sum(), (dataset[mask]['Z']==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "cols=['Index', 'Pt', 'Nt', 'Alt', 'PAlt', 'NAlt']\n",
    "df_cases = pd.DataFrame(columns = cols)\n",
    "\n",
    "for index, case in data.iterrows():\n",
    "    if (case['Z']==1): continue # only process fail cases\n",
    "    k+=1\n",
    "    pimpact=[]\n",
    "    nimpact=[]\n",
    "    for LHS in pRc:\n",
    "        if evaluatepositivecausalrule(LHS,case):\n",
    "            pimpact.extend(positiveimpact(LHS,case))\n",
    "    for LHS in nRc:\n",
    "        if evaluatenegativecausalrule(LHS,pimpact,case):\n",
    "            nimpact.extend(negativeimpact(LHS,pimpact,case))\n",
    "    print ('Case ', k, ': ', case)\n",
    "    Fneg=nimpact\n",
    "    Fpos=pimpact\n",
    "    pimps2=[ p for p in Fpos if (p  in Fneg)]\n",
    "    if len(pimps2)>0:\n",
    "        print ('Negative feature appears also as positive feature, please rerun to compute alternative fair data set')\n",
    "        sys.exit()\n",
    "    Fpos=[ p for p in Fpos if (p not in Fneg)]\n",
    "    Falt=[] # features are in conflict with features in LHS of negative causal rule satisfied by c\n",
    "    for n in Fneg:\n",
    "        falt=alternative(n,case,Fpos,Fneg)\n",
    "        if falt==None:\n",
    "            print ('Error: no alternative feature found for ', n)\n",
    "            exit()\n",
    "        Falt.append(falt)\n",
    "        print ('Negative Repair: replace ', n, ' with ', falt)\n",
    "    Fprev=[] # features satisfied by c that are in conflict with features introduced by LHS of positive causal rule\n",
    "    for p in Fpos:\n",
    "        fprev=prevalternative(p,case)\n",
    "        if fprev==None:\n",
    "            print ('Error: no alternative feature found for ', n)\n",
    "            exit()\n",
    "        Fprev.append(fprev)\n",
    "    Fposnew=resolveconflicts(Fpos,Fprev) # conflicts can only be resolved if Fprev is known to compute the lowest repair distance\n",
    "    for p in Fposnew:\n",
    "        fprev=prevalternative(p,case)\n",
    "        if fprev==None:\n",
    "            print ('Error: no alternative feature found for ', n)\n",
    "            exit()\n",
    "        print ('Positive Repair: replace ', fprev, ' with ', p)\n",
    "\n",
    "    (alt_cases, pos_alt, neg_alt)= findoutcomealternativecase(data,case,Fneg,Falt,Fprev,Fposnew)\n",
    "    df_cases = df_cases.append({'Index' : index, 'Pt' : len(Fposnew), 'Nt' : len(Fneg), 'Alt':alt_cases, 'PAlt':pos_alt, 'NAlt':neg_alt },\n",
    "        ignore_index = True)\n",
    "\n",
    "df_cases.to_csv(\"repair_results.csv\", sep=\";\", index=False)\n",
    "df_cases.describe().to_csv(\"desc.csv\", sep=\";\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
